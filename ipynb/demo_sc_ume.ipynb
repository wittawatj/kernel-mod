{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook to illustrate/test `kmod.mctest.SC_UME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "#%config InlineBackend.figure_format = 'pdf'\n",
    "\n",
    "import kmod\n",
    "import kgof\n",
    "import kgof.goftest as gof\n",
    "# submodules\n",
    "from kmod import data, density, kernel, util\n",
    "from kmod import mctest as mct\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# font options\n",
    "font = {\n",
    "    #'family' : 'normal',\n",
    "    #'weight' : 'bold',\n",
    "    'size'   : 18\n",
    "}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "plt.rc('lines', linewidth=2)\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple 1d Gaussian problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two models $P = \\mathcal{N}(\\mu_p, \\sigma_p^2)$ and $Q = \\mathcal{N}(\\mu_q, \\sigma^2_q)$. The data generating distribution is $R = \\mathcal{N}(0, 1)$.\n",
    "\n",
    "    H_0: P, Q are equally good\n",
    "    H_1: Q is better for approximating R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mp, varp = 1.5, 1\n",
    "# q cannot be the true model. \n",
    "# That violates our assumption and the asymptotic null distribution\n",
    "# does not hold.\n",
    "mq, varq = 1.0, 1\n",
    "\n",
    "# draw some data\n",
    "n = 600 # sample size\n",
    "seed = 8\n",
    "with util.NumpySeedContext(seed=seed):\n",
    "    X = np.random.randn(n, 1)*varp**0.5 + mp\n",
    "    Y = np.random.randn(n, 1)*varq**0.5 + mq\n",
    "    Z = np.random.randn(n, 1)\n",
    "    \n",
    "    datap = data.Data(X)\n",
    "    dataq = data.Data(Y)\n",
    "    datar = data.Data(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(X, color='r', alpha=0.6, normed=True, label='X')\n",
    "plt.hist(Y, color='b', alpha=0.6, normed=True, label='Y')\n",
    "plt.hist(Z, color='k', alpha=0.8, normed=True, label='Z')\n",
    "plt.title('H1: Y is closer to Z')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use random test locations and median heuristic for the Gaussian widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters of the test\n",
    "medxz = util.meddistance(np.vstack((X, Z)), subsample=1000)\n",
    "medyz = util.meddistance(np.vstack((Y, Z)), subsample=1000)\n",
    "k = kernel.KGauss(sigma2=medxz**2)\n",
    "l = kernel.KGauss(sigma2=medyz**2)\n",
    "# 2 sets of test locations\n",
    "J = 2\n",
    "# Jp = J\n",
    "# Jq = J\n",
    "V = util.fit_gaussian_draw(X, J, seed=seed+2)\n",
    "# W = util.fit_gaussian_draw(Y, Jq, seed=seed+3)\n",
    "W = V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a UME test\n",
    "alpha = 0.01 # significance level \n",
    "scume = mct.SC_UME(datap, dataq, k, l, V, W, alpha=alpha)\n",
    "scume.perform_test(datar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the test locations and Gaussian widths\n",
    "\n",
    "Optimize two sets V, W of test locations and two Gaussian widths. Each set is optimized separately by maximizing the power criterion of its corresponding two-sample test problem. Specifically, V is optimized on the power criterion of UME(P, R), and W is optimized on the power criterion of UME(Q,R).\n",
    "\n",
    "Optimizing the two sets in this way does not necessarily give parameters which maximize the test power of the three-sample test that we consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into training/test sets\n",
    "[(datptr, datpte), (datqtr, datqte), (datrtr, datrte)] = \\\n",
    "    [D.split_tr_te(tr_proportion=0.3, seed=85) for D in [datap, dataq, datar]]\n",
    "Xtr, Ytr, Ztr = [D.data() for D in [datptr, datqtr, datrtr]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize optimization parameters.\n",
    "# Initialize the Gaussian widths with the median heuristic\n",
    "medxz = util.meddistance(np.vstack((Xtr, Ztr)), subsample=1000)\n",
    "medyz = util.meddistance(np.vstack((Ytr, Ztr)), subsample=1000)\n",
    "gwidth0p = medxz**2\n",
    "gwidth0q = medyz**2\n",
    "\n",
    "# numbers of test locations in V, W\n",
    "J = 2\n",
    "Jp = J\n",
    "Jq = J\n",
    "\n",
    "# pick a subset of points in the training set for V, W\n",
    "Xyztr = np.vstack((Xtr, Ytr, Ztr))\n",
    "VW = util.subsample_rows(Xyztr, Jp+Jq, seed=73)\n",
    "V0 = VW[:Jp, :]\n",
    "W0 = VW[Jp:, :]\n",
    "\n",
    "# optimization options\n",
    "opt_options = {\n",
    "    'max_iter': 100,\n",
    "    'reg': 1e-4,\n",
    "    'tol_fun': 1e-6,\n",
    "    'locs_bounds_frac': 100,\n",
    "    'gwidth_lb': None,\n",
    "    'gwidth_ub': None,\n",
    "}\n",
    "\n",
    "umep_params, umeq_params = mct.SC_GaussUME.optimize_2sets_locs_widths(\n",
    "    datptr, datqtr, datrtr, V0, W0, gwidth0p, gwidth0q, \n",
    "    **opt_options)\n",
    "(V_opt, gw2p_opt, opt_infop) = umep_params\n",
    "(W_opt, gw2q_opt, opt_infoq) = umeq_params\n",
    "k_opt = kernel.KGauss(gw2p_opt)\n",
    "l_opt = kernel.KGauss(gw2q_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_infoq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data and the learned locations\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(Xtr, color='r', alpha=0.6, normed=True, label='X')\n",
    "plt.hist(Ytr, color='b', alpha=0.6, normed=True, label='Y')\n",
    "plt.hist(Ztr, color='k', alpha=0.8, normed=True, label='Z')\n",
    "for v in V_opt:\n",
    "    plt.plot(v[0], 0, '^', color='magenta', markersize=40)\n",
    "for w in W_opt:\n",
    "    plt.plot(w[0], 0, '*', color='green', markersize=40)\n",
    "    \n",
    "plt.title('H1: Y is closer to Z')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a UME test\n",
    "alpha = 0.01 # significance level \n",
    "scume_opt2 = mct.SC_UME(datpte, datqte, k_opt, l_opt, V_opt, W_opt, alpha=alpha)\n",
    "scume_opt2.perform_test(datrte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw2p_opt, gw2q_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the power criterion of the three-sample test\n",
    "\n",
    "Assume that UME(P, R) and UME(Q, R) share the same set V of J test locations, and the same Gaussian kernel. Optimize V and the Gaussian bandwidth by maximizing the test power criterion of the three-sample test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into training/test sets\n",
    "[(datptr, datpte), (datqtr, datqte), (datrtr, datrte)] = \\\n",
    "    [D.split_tr_te(tr_proportion=0.4, seed=85) for D in [datap, dataq, datar]]\n",
    "Xtr, Ytr, Ztr = [D.data() for D in [datptr, datqtr, datrtr]]\n",
    "Xyztr = np.vstack((Xtr, Ytr, Ztr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimization parameters.\n",
    "# Initialize the Gaussian widths with the median heuristic\n",
    "medxyz = util.meddistance(Xyztr, subsample=1000)\n",
    "gwidth0 = medxyz**2\n",
    "\n",
    "# numbers of test locations in V = W\n",
    "J = 2\n",
    "\n",
    "# pick a subset of points in the training set for V, W\n",
    "Xyztr = np.vstack((Xtr, Ytr, Ztr))\n",
    "V0 = util.subsample_rows(Xyztr, J, seed=75)\n",
    "\n",
    "# optimization options\n",
    "opt_options = {\n",
    "    'max_iter': 100,\n",
    "    'reg': 1e-4,\n",
    "    'tol_fun': 1e-6,\n",
    "    'locs_bounds_frac': 100,\n",
    "    'gwidth_lb': None,\n",
    "    'gwidth_ub': None,\n",
    "}\n",
    "V_opt, gw2_opt, opt_result = mct.SC_GaussUME.optimize_3sample_criterion(\n",
    "    datptr, datqtr, datrtr, V0, gwidth0, **opt_options)    \n",
    "k_opt = kernel.KGauss(gw2_opt)\n",
    "\n",
    "display(opt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a UME test\n",
    "alpha = 0.01 # significance level \n",
    "scume_opt3 = mct.SC_UME(datpte, datqte, k_opt, k_opt, V_opt, V_opt, alpha=alpha)\n",
    "scume_opt3.perform_test(datrte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data and the learned locations\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(Xtr, color='r', alpha=0.6, normed=True, label='X')\n",
    "plt.hist(Ytr, color='b', alpha=0.6, normed=True, label='Y')\n",
    "plt.hist(Ztr, color='k', alpha=0.8, normed=True, label='Z')\n",
    "for v in V_opt:\n",
    "    plt.plot(v[0], 0, '^', color='magenta', markersize=40)\n",
    "    \n",
    "plt.title('H1: Y is closer to Z')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned locations are supposed to show where Q fits (to Z) better than P does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "\n",
    "Normal usage will not need the following code. The following code is here for checking the implementation during the development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the asymptotic distribution of the SC_UME statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_test_samples(n, seed):\n",
    "    \"\"\"\n",
    "    Return datap, dataq, datar\n",
    "    \"\"\"\n",
    "    mp, varp = 0.5, 1\n",
    "    mq, varq = 1, 1\n",
    "\n",
    "    # draw some data\n",
    "    \n",
    "    with util.NumpySeedContext(seed=seed):\n",
    "        X = np.random.randn(n, 1)*varp**0.5 + mp\n",
    "        Y = np.random.randn(n, 1)*varq**0.5 + mq\n",
    "        Z = np.random.randn(n, 1)\n",
    "\n",
    "        datap = data.Data(X)\n",
    "        dataq = data.Data(Y)\n",
    "        datar = data.Data(Z)\n",
    "    return datap, dataq, datar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1003\n",
    "n = 300 # sample size\n",
    "datap, dataq, datar = gen_test_samples(n, seed)\n",
    "X, Y, Z = [a.data() for a in [datap, dataq, datar]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(X, color='r', alpha=0.6, normed=True, label='X')\n",
    "plt.hist(Y, color='b', alpha=0.6, normed=True, label='Y')\n",
    "plt.hist(Z, color='k', alpha=0.8, normed=True, label='Z')\n",
    "plt.title('H1: Y is closer to Z')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters of the test\n",
    "medxz = util.meddistance(np.vstack((X, Z)), subsample=1000)\n",
    "medyz = util.meddistance(np.vstack((Z, Y)), subsample=1000)\n",
    "k = kernel.KGauss(sigma2=medxz**2)\n",
    "l = kernel.KGauss(sigma2=medyz**2)\n",
    "\n",
    "# 2 sets of test locations\n",
    "J = 1\n",
    "Jp = J\n",
    "Jq = J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick a subset of points from the data for the two sets of locations\n",
    "pool3J = np.vstack((X[:J, :], Y[:J, :], Z[:J, :]))\n",
    "X, Y, Z = [np.delete(a, range(J), 0) for a in [X, Y, Z]]\n",
    "datp, datq, datr = [data.Data(a) for a in [X, Y, Z]]\n",
    "assert X.shape[0] == Y.shape[0]\n",
    "assert Y.shape[0] == Z.shape[0]\n",
    "assert Z.shape[0] == n-J\n",
    "assert datp.sample_size() == n-J\n",
    "assert datq.sample_size() == n-J\n",
    "assert datr.sample_size() == n-J\n",
    "\n",
    "# use two sets of locations: V and W\n",
    "VW = util.subsample_rows(pool3J, 2*J, seed+1)\n",
    "# add a little noise to the locations. \n",
    "# Remember the locations have to be drawn from a distribution with a density\n",
    "XYZ = np.vstack((X, Y, Z))\n",
    "# VW = VW + np.random.randn(2*J, X.shape[1])*np.mean(np.std(XYZ, axis=0))\n",
    "with util.NumpySeedContext(seed=seed+8):\n",
    "    VW = VW + np.random.randn(2*J, X.shape[1])*np.mean(np.std(XYZ, axis=0))\n",
    "\n",
    "V = VW[:J, :]\n",
    "W = VW[J:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# V = util.fit_gaussian_draw(X, Jp, seed=seed+2)\n",
    "# W = util.fit_gaussian_draw(Y, Jq, seed=seed+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times to create a new problem (draw new samples)\n",
    "trials = 300\n",
    "null_stats = np.zeros(trials)\n",
    "alpha = 0.05 # significance level \n",
    "\n",
    "for t in range(trials):\n",
    "    datap, dataq, datar = gen_test_samples(n, seed=(t*17)%(2**31))\n",
    "    # create a UME test\n",
    "    \n",
    "    scume = mct.SC_UME(datap, dataq, k, l, V, W, alpha=alpha)\n",
    "    null_stats[t] = scume.compute_stat(datar)\n",
    "\n",
    "# use the data in the last trial to perform test\n",
    "results = scume.perform_test(datar)\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the parameters of the asymptotic null distribution\n",
    "_, var_h0 = scume.get_H1_mean_variance(datar, return_variance=True)\n",
    "dom =  np.linspace(np.min(null_stats)-1, np.max(null_stats)+2, 300)\n",
    "ph0_values = stats.norm.pdf(dom, loc=0, scale=var_h0**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the null stats\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(dom, ph0_values, 'r-', label='Asymp. null dist.')\n",
    "plt.hist(null_stats, label='Empirical ground truth', alpha=0.7, bins=15, normed=True);\n",
    "# plt.hist(sim_stats, label='Asymptotic', alpha=0.7, bins=15, normed=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $H_0$ is true, the asymptotic null distribution is expected to be to the right of the empirically obtained statistics. This means that we will have type-I error which is lower than $\\alpha$, but lose a bit of test power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
